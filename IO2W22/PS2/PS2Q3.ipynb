{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem Set 2 Question 3\n",
    "\n",
    "Each time period, Harold Zurcher chooses to perform maintenance on the bus, or to replace the engine. His flow utilities are given by the following function:\n",
    "\n",
    "$$\n",
    "    u\\left(x_{t}, d_{t}\\right) + \\epsilon_{a, t} \\; = \\; \\begin{cases}\n",
    "-\\theta_{1} x_{t}-\\theta_{2}\\left(\\frac{x_{t}}{100}\\right)^{2}+\\epsilon_{0, t} \\quad & \\text { if } d_{t}=0 \\\\\n",
    "-\\theta_{3}+\\epsilon_{1, t} \\quad & \\text { if } d_{t}=1 \\end{cases}\n",
    "$$\n",
    "\n",
    "where $x_{t}$ is the current mileage of the bus, $d_{t}$ is the choice of Harold Zurcher, and $\\theta$ is a vector of parameters. Each choice also contains unobserved utility $\\epsilon_{a, t}$ that are distributed independent T1EV.\n",
    "\n",
    "Harold Zurcher maximizes his lifetime discounted utility, discounted by $\\beta$, where the state $x_{t}$ evolves according to\n",
    "$$\n",
    "p\\left(x_{t+1} \\mid x_{t}, d_{t}\\right) \\; = \\; \\begin{cases}g\\left(x_{t+1}-0\\right) \\quad & \\text { if } d_{t}=1 \\\\ g\\left(x_{t+1}-x_{t}\\right) \\quad & \\text { if } d_{t}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "That is, replacing the engine regenerates the mileage to 0.\n",
    "\n",
    "------------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 1\n",
    "\n",
    "We can detect engine replacements from the data when the mileage decreases from one period to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import logsumexp\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data = pd.read_csv('ps2_ex3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.rename(columns={'milage': 'mileage'}, inplace=True)\n",
    "data['mileage_old'] = data['mileage'].shift(periods=1, fill_value=0)\n",
    "\n",
    "data['replace'] = (data['mileage_old'] - data['mileage'] > 0) * 1\n",
    "\n",
    "x_max = data['mileage'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "From the state transition process, we see that $ \\left( x_{t+1} \\perp \\epsilon_{a, t}, \\epsilon_{a, t+1} \\right) \\mid x_t, d_t$. This implies that\n",
    "\n",
    "\\begin{align*}\n",
    "    p\\left(x_{t+1}, \\epsilon_{a,t+1} \\mid x_{t}, d_{t}, \\epsilon_{a,t} \\right) \\; & = \\; p\\left(x_{t+1} \\mid x_{t}, d_{t}, \\epsilon_{a,t} \\right) \\cdot p\\left(x_{t+1}, \\epsilon_{a,t+1} \\mid x_{t}, d_{t}, \\epsilon_{a,t} \\right) \\\\\n",
    "    & = \\; p\\left(x_{t+1} \\mid x_{t}, d_{t}, \\epsilon_{a,t} \\right) \\cdot p\\left( \\epsilon_{a,t+1} \\mid x_{t}, d_{t}, \\epsilon_{a,t} \\right) \\\\\n",
    "    & = \\; p\\left(x_{t+1} \\mid x_{t}, d_{t}  \\right) \\cdot p\\left( \\epsilon_{a,t+1} \\mid \\epsilon_{a,t} \\right) \\\\\n",
    "    & = \\; p\\left(x_{t+1} \\mid x_{t}, d_{t}  \\right) \\cdot g(\\epsilon_{t+1})\n",
    "\\end{align*}\n",
    "\n",
    "------------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 3\n",
    "\n",
    "The following function creates the transition matrix for any level of discretization of the state space. We use the trick from Rust's paper to set $\\mathbb{P}(x' | x, 1) = \\mathbb{P}(x'|0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_trans_mat(dataset, length_inter):\n",
    "\n",
    "    \"\"\"\n",
    "    This function estimates the transition matrix pertaining to the evolution of the observed state variable x_t.\n",
    "    :param dataset: The data with mileage information.\n",
    "    :param length_inter: The length of chunk to be used to discretize the domain of the state variable.\n",
    "    :return: A transition matrix of size (number_of_intervals, number_of_intervals, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    k = length_inter\n",
    "    x_max_round = round(x_max / k) + 1\n",
    "    trans_mat = np.zeros((x_max_round, x_max_round, 2)\n",
    ")\n",
    "    # Create transition matrix for a = 0\n",
    "    i = 0\n",
    "\n",
    "    # Extract data with the same decision: replace (1) or not replace (0).\n",
    "    data_x = dataset.query('replace == {}'.format(i))\n",
    "\n",
    "    # Rounding mileage using the interval size provided.\n",
    "    data_x['mileage'] = k * round(data_x['mileage'] / k)\n",
    "    data_x['mileage_old'] = k * round(data_x['mileage_old'] / k)\n",
    "\n",
    "    # Constructing the counts of transition from each value of mileage_old to mileage.\n",
    "    data_x = data_x.groupby(['mileage_old', 'mileage'])['mileage'].count().rename('count').reset_index()\n",
    "    data_x = data_x.merge(data_x.groupby(['mileage_old'])['count'].sum().rename('total').reset_index(), on='mileage_old')\n",
    "\n",
    "    # Construct transition probabilities from the counts of transitions.\n",
    "    data_x['prob'] = data_x['count']/data_x['total']\n",
    "\n",
    "    # Encode probability data into a transition matrix.\n",
    "    trans_mat[(data_x['mileage_old']/k).astype(int),\n",
    "              (data_x['mileage']/k).astype(int), i] = data_x['prob']\n",
    "\n",
    "    # Add 1s in appropriate cells to ensure that the Markov transition matrices are valid.\n",
    "    for inter in range(x_max_round):\n",
    "\n",
    "        if trans_mat[inter, :, i].sum() == 0:\n",
    "\n",
    "            if i == 0 and inter < x_max_round - 1: # Don't replace engine, mileage goes to next state\n",
    "                trans_mat[inter, inter + 1, i] = 1\n",
    "\n",
    "            elif i == 0 and inter == x_max_round - 1:  # Don't replace, boundary\n",
    "                trans_mat[inter, inter, i] = 1\n",
    "\n",
    "    # Set the transition matrix for a = 1 to values from Pr(x'|0, 0)\n",
    "    trans_mat[:, :, 1] = np.repeat(trans_mat[0, :, 0][None, :],\n",
    "                                   repeats=x_max_round, axis=0)\n",
    "    # Need to replace zeros with infinitesimal values to not run into\n",
    "    # overflow issues.\n",
    "    trans_mat = trans_mat + 1e-6\n",
    "    trans_mat = trans_mat / trans_mat.sum(axis=1)[:, None, :]\n",
    "\n",
    "    return data_x, torch.tensor(trans_mat, dtype=torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, trans_matrix = generate_trans_mat(dataset=data, length_inter=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 4\n",
    "\n",
    "The Bellman equation for this problem is given by\n",
    "\n",
    "\\begin{align*}\n",
    "V(x, \\epsilon) \\; &= \\; \\max_{a \\in \\{0, 1\\}}\\left\\{u(x, a)+\\epsilon_{j}+\\beta \\int_{x'} \\int_{\\epsilon^{\\prime}} V\\left(x', \\epsilon^{\\prime}\\right) p\\left(\\epsilon^{\\prime}\\right) \\, \\text{d} \\epsilon' f(x' \\mid x, a) \\, \\text{d} x'\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "We can define the expected value function $w(x) \\; \\equiv \\; \\int_{\\epsilon^{\\prime}} V\\left(x, \\epsilon^{\\prime}\\right) p\\left(\\epsilon^{\\prime}\\right) \\, \\text{d} \\epsilon'$. Taking the expected value with respect to $\\epsilon$ on both sides of the Bellman, we get that\n",
    "\n",
    "\\begin{align*}\n",
    "w(x) \\; &= \\;  \\int_{\\epsilon} \\max_{a \\in \\{0, 1\\}}\\left\\{u(x, a)+\\epsilon +\\beta \\int_{x'} \\int_{\\epsilon^{\\prime}} V\\left(y, \\epsilon^{\\prime}\\right) p\\left(\\epsilon^{\\prime}\\right) \\, \\text{d} \\epsilon' f(x' \\mid x, a) \\, \\text{d} x'\\right\\} p\\left(\\epsilon \\right) \\, \\text{d} \\epsilon \\\\\n",
    "& = \\;  \\int_{\\epsilon} \\max_{a \\in \\{0, 1\\}}\\left\\{ u(x, a) + \\epsilon_a +\\beta \\int_{x'} w(x') f(x' \\mid x, a) \\, \\text{d} x'\\right\\} p\\left(\\epsilon \\right) \\, \\text{d} \\epsilon \\\\\n",
    "& = \\; \\log \\sum_{a = 0}^1 \\exp \\left\\{ u(x, a) +\\beta \\int_{x'} w(x') f(x' \\mid x, a) \\, \\text{d} x' \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "We can then take an expectation with respect to $x$ for a given choice $d$ to arrive at the choice-specific value function, defined as $\\mathcal{V}(x, d) \\equiv \\int_{x'} w(x') f(x' \\mid x, a) \\, \\text{d} x'$.\n",
    "\n",
    "$$\n",
    "    \\mathcal{V}(y, d) \\; = \\; \\int_{x} \\log \\sum_{a = 0}^1 \\exp \\left\\{ u(x, a) + \\beta \\,  \\mathcal{V}(x, a) \\right\\} f(x \\mid y, d) \\, \\text{d} x\n",
    "$$\n",
    "\n",
    "Since we have discretized the state space, we can write that\n",
    "\n",
    "$$\n",
    "     \\mathcal{V}(y, d) \\; = \\; \\sum_{x} \\left( \\log \\sum_{a = 0}^1 \\exp \\left\\{ u(x, a) + \\beta \\,  \\mathcal{V}(x, a) \\right\\} \\right) p(x \\mid y, d)\n",
    "$$\n",
    "\n",
    "Note that $p(x \\mid y, d)$ comes from the transition matrix derived in Part 3, and that $\\mathcal{V}$ is a function $\\boldsymbol{\\theta}$.\n",
    "\n",
    "------------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 5\n",
    "\n",
    "The conditional choice probability for replacing the engine is given by\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(a = 1 \\mid x; \\boldsymbol{\\theta}) \\; = \\; \\mathbb{P}\\left[ \\mathcal{V}(x, 1) + \\epsilon_1  > \\mathcal{V}(x, 0) + \\epsilon_0 \\right] \\; = \\; \\frac{\\exp \\left(\\mathcal{V}(x, 1) \\right)}{\\exp \\left(\\mathcal{V}(x, 1) \\right) + \\exp \\left(\\mathcal{V}(x, 0) \\right)}\n",
    "$$\n",
    "\n",
    "Likewise, we can derive that\n",
    "\n",
    "$$\n",
    "    \\mathbb{P} \\left(a = 1 \\mid x ; \\boldsymbol{\\theta} \\right) \\; = \\;  \\frac{\\exp \\left(\\mathcal{V}(x, 0) \\right)}{\\exp \\left(\\mathcal{V}(x, 1) \\right) + \\exp \\left(\\mathcal{V}(x, 0) \\right)}\n",
    "$$\n",
    "\n",
    "---------\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 6-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_utility(theta, length_inter):\n",
    "\n",
    "    k = length_inter\n",
    "    x_max_round = round(x_max / k) + 1\n",
    "    state_mat = torch.tensor(np.arange(0, k * x_max_round), dtype=torch.double)\n",
    "\n",
    "    utility_mat = torch.ones((x_max_round * k, 2), dtype=torch.double) * - theta[2]\n",
    "\n",
    "    utility_mat[:, 0] = - theta[0] * state_mat - theta[1] * (state_mat ** 2)/10000\n",
    "\n",
    "    utility_mat = torch.reshape(utility_mat, shape=(x_max_round, k, 2))\n",
    "    utility_mat = utility_mat.mean(axis=1)\n",
    "\n",
    "    return utility_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def solve_bellman(theta, beta, trans_mat, length_inter, delta, tol):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that derives the fixed-point solution to the Bellman equation characterizing the\n",
    "    behavior of the choice specific value function.\n",
    "    :param theta: Parameters in the utility functions.\n",
    "    :param beta: Discount factor.\n",
    "    :param trans_mat: Markov transition matrix corresponding to the states.\n",
    "    :param length_inter: Discretization level used to characterize the state space.\n",
    "    :param tol: Tolerance used to determine the solution of the fixed point problem.\n",
    "    :param delta: Relaxation parameter to arrive at the fixed point solution.\n",
    "    :return: The fixed-point solution to the EVF Bellman, the CCPs associated with the state space,\n",
    "     and the number of iterations taken to arrive at the fixed point.\n",
    "    \"\"\"\n",
    "\n",
    "    evf = torch.ones((trans_matrix.shape[0], 2), dtype=torch.double)\n",
    "    utility_mat = generate_utility(theta=theta, length_inter=length_inter)\n",
    "\n",
    "    error, count, max_count = 1, 0, 100000\n",
    "\n",
    "    while error > tol and count < max_count:\n",
    "\n",
    "        evf2 = torch.einsum('j, ijk -> ik', logsumexp(input=utility_mat + beta * evf, dim=1), trans_mat)\n",
    "\n",
    "        error = torch.norm(evf2 - evf)\n",
    "\n",
    "        evf = delta * evf2 + (1 - delta) * evf\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print('[{}]: error = {}'.format(count, error.detach().numpy()))\n",
    "\n",
    "    ccp = torch.exp(evf - logsumexp(input=evf, dim=1)[:, None])\n",
    "\n",
    "    return evf, ccp, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EVF, CCP, _ = solve_bellman(theta=np.array([-0.1, 10, -1]), beta=0.999, trans_mat=trans_matrix, length_inter=K, delta=0.999, tol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "The MLE chooses the parameter set that maximizes the likelihood associated with the data.\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{\\boldsymbol{\\theta}} \\; &= \\; \\underset{ \\boldsymbol{\\theta} }{\\arg \\max} \\prod_{t=2}^{T}\\left\\{\\prod_{j \\in \\{0, 1\\} } \\mathbb{P}\\left(a_{t}=j \\mid x_{t} ; \\boldsymbol{\\theta} \\right)^{\\mathbf{1}\\left\\{a_{t}=j\\right\\}}\\right\\} \\mathbb{P}\\left(x_{t} \\mid x_{t-1}, a_{t-1} \\right) \\\\\n",
    "&= \\; \\underset{ \\boldsymbol{\\theta} }{\\arg \\max} \\sum_{t=2}^{T}\\left\\{\\sum_{j \\in \\{0, 1\\}} \\mathbf{1}\\left\\{ a_{t}=j\\right\\} \\; \\ln \\mathbb{P}\\left(a_{t}=j \\mid x_{t} ; \\boldsymbol{\\theta} \\right) \\right\\} + \\sum_{t=2}^{T} \\ln \\mathbb{P}\\left(x_{t} \\mid x_{t-1}, a_{t-1}  \\right)\n",
    "\\end{align*}\n",
    "\n",
    "The following function `evaluate_likelihood` first solves the Bellman equation for the expected value function at the given parameter values $\\boldsymbol{\\theta}$ for a specified discount factor and then evaluates the log-likelihood associated with the data using the conditional choice probabilities derived from the fixed point solution to the Bellman equation as well as the provided Markov matrices for state transition probabilities associated with each possible choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_likelihood(dataset, theta, beta, trans_mat, length_inter, delta, tol):\n",
    "    \"\"\"\n",
    "    A function that computes the loglikelihood associated with the provided choice data based on\n",
    "    conditional choice probabilities derived from the fixed point of the Bellman equation involving\n",
    "    the expected value function.\n",
    "\n",
    "    :param dataset: The data with mileage and choice information.\n",
    "    :param theta: Parameters in the utility functions.\n",
    "    :param beta: Discount factor.\n",
    "    :param trans_mat: Markov transition matrix corresponding to the states.\n",
    "    :param length_inter: Discretization level used to characterize the state space.\n",
    "    :param tol: Tolerance used to determine the solution of the fixed point problem.\n",
    "    :param delta: Relaxation parameter to arrive at the fixed point solution.\n",
    "    :return: The likelihood value associated with\n",
    "    \"\"\"\n",
    "\n",
    "    k = length_inter\n",
    "\n",
    "    # CCP solution derived from the solution to the Bellman equation.\n",
    "    _, ccp, _ = solve_bellman(theta=theta, beta=beta, trans_mat=trans_mat,\n",
    "                                length_inter=length_inter, delta=delta, tol=tol)\n",
    "\n",
    "    # Converting mileage values to indices used in the CCP and transition matrices.\n",
    "    mileage_index = np.array((dataset['mileage'] / k).astype(int)[1:])\n",
    "    mileage_old_index = np.array((dataset['mileage_old'] / k).astype(int)[1:])\n",
    "    replace = np.array((dataset['replace']).astype(int)[1:])\n",
    "\n",
    "    # Likelihood contribution from the conditional choice probabilities.\n",
    "    ll_term_1 = torch.sum(torch.log(ccp[mileage_index, replace]))\n",
    "\n",
    "    # Likelihood contribution from the Markov transition probabilities.\n",
    "    ll_term_2 = torch.sum(torch.log(trans_mat[mileage_index, mileage_old_index, replace]))\n",
    "\n",
    "    return ll_term_1 + ll_term_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10\n",
    "\n",
    "The following function takes an initial guess for the model parameters and estimates them from the dataset using the NFXP + MLE approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_mle(init_guess, dataset, beta, trans_mat, length_inter):\n",
    "\n",
    "    # Initialize the optimization variables for the ML procedure.\n",
    "    theta_hat = Variable(init_guess, requires_grad=True)\n",
    "\n",
    "    init_guess2 = init_guess.detach().clone().numpy()\n",
    "\n",
    "    # Define the adaptive gradient descent optimizer used to find the estimates.\n",
    "    opt = optim.Adam([theta_hat], lr=0.1)\n",
    "\n",
    "    # Define the objective function.\n",
    "    log_lik = lambda x: evaluate_likelihood(dataset=dataset, theta=x, beta=beta, trans_mat=trans_mat, length_inter=length_inter, delta=0.999, tol=1e-9)\n",
    "\n",
    "    # Loss vector\n",
    "    loss_list = list()\n",
    "\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        opt.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "        # Compute the objective at the current parameter values.\n",
    "        loss = - log_lik(theta_hat)\n",
    "        loss.backward() # Gradient computed.\n",
    "        opt.step()\n",
    "\n",
    "        # Store value of loss.\n",
    "        loss_list.append(loss.detach().clone())\n",
    "\n",
    "    return {'init': init_guess2,\n",
    "            'final': theta_hat.detach().clone().numpy(),\n",
    "             'loss': loss_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create grid of initial conditions used in ML estimation\n",
    "\n",
    "init_vec = np.linspace(start=-5, stop=10, num=21)\n",
    "input_grid = np.array(np.meshgrid(init_vec, init_vec, init_vec)).T.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "\n",
    "# Run ML estimation for each initial condition.\n",
    "for ix in range(input_grid.shape[0]):\n",
    "\n",
    "    result_dict = run_mle(init_guess=torch.tensor(input_grid[ix, :]),\n",
    "                          dataset=data, beta=0.999,\n",
    "                          trans_mat=trans_matrix, length_inter=K)\n",
    "\n",
    "    results_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the estimates with the lowest negative log-likelihood. The ML estimates for $\\boldsymbol{\\theta}$ are:\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix} \\widehat{\\theta}_1 \\\\ \\widehat{\\theta}_2 \\\\ \\widehat{\\theta}_3 \\end{pmatrix} \\; = \\; \\begin{pmatrix} 0.8122 \\\\  9.9459 \\\\ -2.2145 \\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8122,  9.9459, -2.2145], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}