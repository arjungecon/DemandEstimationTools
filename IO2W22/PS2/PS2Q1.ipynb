{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem Set 2 Question 1\n",
    "\n",
    "Consider a static entry game between $J$ firms operating in $T$ markets. Upon entry, the profit of firm $j$ in market $t$ depends on the number of entrants and is given by:\n",
    "\n",
    "$$\n",
    "\\pi_{j t}\\left(n_{-j t}\\right) \\; = \\; x_{j t} \\beta-\\phi_{j}-\\delta_{j} \\log \\left(1+n_{-j t}\\right)+\\epsilon_{j t}\n",
    "$$\n",
    "\n",
    "where $x_{j t}$ are market-firm specific profits shifter, $\\phi_{j}$ is a firm specific fixed cost, $n_{-j t}$ is the number of competitors in market $t$ and $\\epsilon_{j t} \\sim F$ is an idiosyncratic shock. Given $n_{-j t}$ firms, firm $j$ enters if and only if $\\pi_{j t} \\geq 0$.\n",
    "\n",
    "-------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 1\n",
    "\n",
    "In the case with $J = 2$, we can characterize the payoff matrix as follows:\n",
    "\n",
    "|              |                                                          $\\text{E}_2$                                                          |                 $\\text{O}_2$                  |\n",
    "|:------------:|:------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------:|\n",
    "| $\\text{E}_1$ |    $(x_{1 t} \\beta-\\phi_{1}-\\delta_{1} \\log 2 +\\epsilon_{1 t},  x_{2 t} \\beta-\\phi_{2}-\\delta_{2} \\log 2 +\\epsilon_{2 t} )$    | $(x_{1 t} \\beta-\\phi_{1} +\\epsilon_{1 t}, 0)$ |\n",
    "| $\\text{O}_1$ |                                         $(0, x_{2 t} \\beta-\\phi_{2} +\\epsilon_{2 t})$                                          |                   $(0, 0)$                    |\n",
    "\n",
    "The pure-strategy Nash equilibrium realization depends on the parameters and the realizations of $\\epsilon_{i, j}$ 's. Firm $i$ enters market $t$ if it earns a non-negative profit, that is, $\\mathbf{1}\\left\\{\\pi_{it} \\geq 0\\right\\}$. Hence, we can summarize the realizations of Nash equilibrium and its corresponding region of $\\left(\\epsilon_{1, j}, \\epsilon_{2, j}\\right)$ as follows:\n",
    "\n",
    "- $\\mathcal{R}_{3} \\; = \\; \\left\\{\\left(\\text{O}_{1}, \\text{O}_{2} \\right) \\text{ is observed } \\right\\}=\\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right) \\, : \\, x_{1 t} \\beta-\\phi_{1}+\\epsilon_{1 t} <0 \\text{ and } x_{2 t} \\beta-\\phi_{2} + \\epsilon_{2 t} <0 \\right\\}$\n",
    "- $\\mathcal{R}_{1}=\\left\\{\\left(\\text{E}_{1}, \\text{E}_{2}\\right) \\text{ is observed} \\right\\} \\; = \\; \\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right):  x_{1 t} \\beta-\\phi_{1}-\\delta_1 \\log 2 +\\epsilon_{1 t}>0 \\text{ and } x_{2 t} \\beta-\\phi_{2} - \\delta_2 \\log 2 + \\epsilon_{2 t} >0\\right\\}$\n",
    "- $\\mathcal{R}_{2}+\\mathcal{R}_{5} \\; = \\; \\left\\{\\left(\\text{E}_{1}, \\text{O}_{2}\\right) \\text{ is observed} \\right\\} = \\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right): x_{1 t} \\beta-\\phi_{1}+\\epsilon_{1 t} >0 \\text{ and } x_{2 t} \\beta-\\phi_{2} - \\delta_2 \\log 2 + \\epsilon_{2 t} <0\\right\\}$\n",
    "- $\\mathcal{R}_{4}+\\mathcal{R}_{5} \\; = \\; \\left\\{\\left(\\text{O}_{1}, \\text{E}_{2}\\right) \\text{ is observed} \\right\\} \\;  \\; =\\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right):x_{1 t} \\beta-\\phi_{1} - \\delta_1 \\log 2 +\\epsilon_{1 t} <0 \\text{ and } x_{2 t} \\beta-\\phi_{2} + \\epsilon_{2 t}>0\\right\\}$\n",
    "\n",
    "-------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 2\n",
    "\n",
    "The overlapping region of likelihood between $\\left(\\text{E}_{1}, \\text{O}_{2}\\right)$ and $\\left(\\text{E}_{2}, \\text{O}_{1}\\right)$ generates problems for identification. Even if we have pinned down the parameters $(\\beta, \\phi_1, \\phi_2, \\delta_1, \\delta_2)$, a pair $(\\epsilon_{1t}, \\epsilon_{2t})$ that lies in the region $\\mathcal{R}_5$ can either generate $\\left( \\text{E}_{1}, \\text{O}_{2} \\right)$ or $\\left(\\text{E}_{2}, \\text{O}_{1}\\right)$ as an equilibrium outcome and predicting which of the two occurs is impossible without further assumptions. Specifying the likelihood function given the data for all the model parameters now will involve double-counting region $\\mathcal{R}_5$ since this will be incorporated in the likelihood terms for both $\\left(\\text{E}_{1}, \\text{O}_{2}\\right)$ and $\\left(\\text{E}_{2}, \\text{O}_{1}\\right)$. The sum of the probabilities will exceed 1, and the resulting likelihood is not well-defined.\n",
    "\n",
    "-------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 3\n",
    "\n",
    "The assumption of sequential entry simplifies the problem. Now the region $\\mathcal{R}_5$ can be attributed to the first entrant in the market, i.e. firm 1.\n",
    "\n",
    "- $\\mathcal{R}_{3} \\; = \\; \\left\\{\\left(\\text{O}_{1}, \\text{O}_{2} \\right) \\text{ is observed } \\right\\}=\\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right) \\, : \\, x_{1 t} \\beta-\\phi_{1}+\\epsilon_{1 t} <0 \\text{ and } x_{2 t} \\beta-\\phi_{2} + \\epsilon_{2 t} <0 \\right\\}$\n",
    "- $\\mathcal{R}_{1}=\\left\\{\\left(\\text{E}_{1}, \\text{E}_{2}\\right) \\text{ is observed} \\right\\} \\; = \\; \\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right):  x_{1 t} \\beta-\\phi_{1}-\\delta_1 \\log 2 +\\epsilon_{1 t}>0 \\text{ and } x_{2 t} \\beta-\\phi_{2} - \\delta_2 \\log 2 + \\epsilon_{2 t} >0\\right\\}$\n",
    "- $\\mathcal{R}_{2}+\\mathcal{R}_{5} \\; = \\; \\left\\{\\left(\\text{E}_{1}, \\text{O}_{2}\\right) \\text{ is observed} \\right\\} = \\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right): x_{1 t} \\beta-\\phi_{1}+\\epsilon_{1 t} >0 \\text{ and } x_{2 t} \\beta-\\phi_{2} - \\delta_2 \\log 2 + \\epsilon_{2 t} <0\\right\\}$\n",
    "- $\\mathcal{R}_{4} \\; = \\; \\left\\{\\left(\\text{O}_{1}, \\text{E}_{2}\\right) \\text{ is observed} \\right\\} \\;  \\; =\\left\\{\\left(\\epsilon_{1t}, \\epsilon_{2t}\\right): x_{1 t} \\beta-\\phi_{1} - \\delta_1 \\log 2 +\\epsilon_{1 t} <0 \\text{ and } x_{2 t} \\beta-\\phi_{2} + \\epsilon_{2 t}>0\\right\\}$\n",
    "\n",
    "The issue with $\\mathcal{R}_5$ being counted twice is now resolved with the addition of sequential entry, and therefore, the likelihood is well-defined.\n",
    "\n",
    "-------\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "### Part 4\n",
    "\n",
    "Assume we have market level data on $T$ markets. For each market $t$, we only observe the number of firms $n_{t}$ and some market level characteristic $x_{t}$. Assume that firms are homogeneous and play a sequential entry game in each market with the same entry profits as before:\n",
    "\n",
    "$$\n",
    "\\pi_{t}\\left(n_{t}\\right) \\; = \\; x_{t} \\beta-\\phi-\\delta \\log \\left(n_{t}\\right)+\\epsilon_{t}\n",
    "$$\n",
    "\n",
    "$n_{t}$ is a Nash Equilibrium under the sequential entry assumption. All firms are equally profitable and therefore we don't need the additional assumption of entrance in order of profitability as in the Berry (1992) case.\n",
    "\n",
    "We assume that $\\epsilon_{t} \\overset{\\text{i.i.d.}}{\\sim} N(0,1)$ and estimate the parameters $(\\beta, \\phi, \\delta)$ from the data in `pset2_ex1.csv` using MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributions as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ps2_ex1.csv')\n",
    "\n",
    "n, x = np.array(data['n']), np.array(data['x'])\n",
    "n, x = torch.tensor(n), torch.tensor(x)\n",
    "\n",
    "max_N, max_T = n.max(), n.size(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters for the ML optimization.\n",
    "beta = Variable(torch.zeros(1) + 0.04, requires_grad=True)\n",
    "phi = Variable(torch.ones(1), requires_grad=True)\n",
    "delta = Variable(torch.ones(1) * 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The i.i.d. normal assumption on the unobserved term allows for ordered probit estimation. This means that the likelihood of observing $n_t$ firms is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}[\\text{observing } n_t \\text{ firms}] \\; & = \\; \\mathbb{P}\\left[\\pi_{t}\\left(n_{t}\\right) > 0,  \\pi_{t}\\left(n_{t} + 1 \\right) < 0 \\right] \\\\\n",
    "    & = \\; \\mathbb{P} \\left[x_{t} \\beta-\\phi-\\delta \\log \\left(n_{t}\\right)+\\epsilon_{t} > 0, x_{t} \\beta-\\phi-\\delta \\log \\left(n_{t} + 1\\right)+\\epsilon_{t} < 0 \\right] \\\\\n",
    "    & = \\;  \\mathbb{P} \\left[-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t}\\right) < \\epsilon_{t}  < -x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t} + 1\\right)  \\right] \\\\\n",
    "    & = \\; \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t} + 1\\right) \\right) - \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t}\\right) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "The log-likelihood for a single observation $(x_t, n_t)$ at time $t$ is given by\n",
    "$$\n",
    "    \\log \\ell (\\beta, \\delta, \\phi | x_t, n_t) \\; = \\; \\sum_{n = 1}^N \\mathbf{1}\\left\\{ n_t = n \\right\\} \\log \\left( \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t} + 1\\right) \\right) - \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t}\\right) \\right)\\right)\n",
    "$$\n",
    "\n",
    "This implies that the log likelihood corresponding to the entire dataset is\n",
    "\n",
    "$$\n",
    "    \\mathcal{L} (\\beta, \\delta, \\phi \\mid \\{x_t, n_t \\}) \\; = \\; \\sum_{t = 1}^T \\sum_{n = 1}^N \\mathbf{1}\\left\\{ n_t = n \\right\\} \\log \\left( \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t} + 1\\right) \\right) - \\Phi \\left(-x_{t} \\beta+\\phi+\\delta \\log \\left(n_{t}\\right) \\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_lik(b, d, p):\n",
    "\n",
    "    term1 = td.Normal(loc=0, scale=1).cdf(-x * b + p + d * torch.log(n + 1))\n",
    "    term2 = td.Normal(loc=0, scale=1).cdf(-x * b + p + d * torch.log(n))\n",
    "\n",
    "    obj = torch.log(term1 - term2).sum()\n",
    "\n",
    "    return obj\n",
    "\n",
    "# Define the adaptive gradient descent optimizer used to find the estimates.\n",
    "opt = optim.Adam([beta, phi, delta], lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(2000):\n",
    "\n",
    "    opt.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss = - log_lik(beta, delta, phi)\n",
    "    loss.backward() # Gradient computed.\n",
    "    opt.step()      # Update parameter values using gradient descent.\n",
    "\n",
    "    print('Step {} : Loss = {}'.format(epoch, loss.detach().clone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Store parameters as tuple\n",
    "theta = (beta, delta, phi)\n",
    "# Fisher Information matrix.\n",
    "info_mle = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta))\n",
    "# Compute variance matrix.\n",
    "var_mle = torch.inverse(info_mle)/max_T\n",
    "# Compute standard errors.\n",
    "std_mle = np.sqrt(np.diag(var_mle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | Estimate |     Std. Error      |\n",
    "|:--------:|:--------:|:-------------------:|\n",
    "| $\\beta$  |  2.1063  |     0.01300029      |\n",
    "| $\\delta$ | 10.3704  |     0.06390927      |\n",
    "|  $\\phi$  |  1.5480  |     0.02433986      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
