{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical Optimization using PyTorch\n",
    "\n",
    "In this Jupyter notebook, I perform some basic econometric optimization routines using the PyTorch package.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.distributions as td"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Basic Optimization Problem\n",
    "\n",
    "Consider the following problem:\n",
    "\n",
    "$$\n",
    "    \\min_{x} \\quad 2 x^2 - 7 x + 6\n",
    "$$\n",
    "\n",
    "We use the `torch.tensor` method to store parameters, constants and matrices. However, the parameters over which we are optimizing the objective function require an additional option `requires_grad=True` so that PyTorch knows how to perform the backpropagation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical optimization solution: tensor([1.7500], requires_grad=True)\n",
      "Analytic optimization solution: 1.75\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-2.45], requires_grad=True)\n",
    "\n",
    "optimizer = optim.Adam([x], lr=0.05)\n",
    "\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = (2 * x**2 - 7 * x + 6)\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Numerical optimization solution:\", x)\n",
    "print(\"Analytic optimization solution:\", 7/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`optimizer.step()` moves the parameter in the direction that minimizes the objective function using the gradient computed in the `.backward()` operation.\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "## Maximum Likelihood Estimation Part 1\n",
    "\n",
    "In this example, consider some observations $\\{X_i \\mid i = 1, \\ldots, N\\}$ drawn from a normal distribution with mean $\\mu$ and variance $\\sigma^2$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "N = 1000\n",
    "μ, σ = 0.05, 0.1\n",
    "x_data = μ + torch.randn(N) * σ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the ML estimates maximize the log-likelihood function, or correspondingly minimize the negative log-likelihood. We can set the mean and standard deviation as PyTorch `Variable`s and ask the optimizer to minimize the objective accordingly.\n",
    "\n",
    "$$\n",
    "    \\widehat{\\boldsymbol{\\theta}} \\; \\equiv \\; \\begin{pmatrix} \\widehat{\\mu} \\\\ \\widehat{\\sigma} \\end{pmatrix} \\quad \\in \\;\\; \\underset{\\begin{pmatrix} \\mu \\\\ \\sigma \\end{pmatrix}}{\\arg \\min} -\\frac{N}{2} \\log (2 \\pi) - N \\log \\sigma - \\frac{1}{2} \\sum_{i = 1}^N  {\\left( \\frac{X_i - \\mu}{\\sigma} \\right)}^2\n",
    "$$\n",
    "\n",
    "I used `torch.distribution`'s inbuilt log-likelihood method corresponding to a Normal distribution to define the sum of the log-likelihoods over the set of observations.\n",
    "\n",
    "The subpackage `torch.optim` implements various optimization algorithms. To construct an `Optimizer`, we provide an iterable containing the parameters (here, $\\mu$ and $\\sigma$) to optimize. Then, we can specify optimizer-specific options such as the learning rate, weight decay, etc. I chose the Adam algorithm with a small learning rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set parameters for the ML optimization.\n",
    "mu_hat = Variable(torch.zeros(1), requires_grad=True)\n",
    "sigma_hat = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "# Define the objective function.\n",
    "def log_lik(mu, sigma):\n",
    "    return td.Normal(loc=mu, scale=sigma).log_prob(x_data).sum()\n",
    "\n",
    "# Define the adaptive gradient descent optimizer used to find the estimates.\n",
    "opt = optim.Adam([mu_hat, sigma_hat], lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "\n",
    "    opt.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss = - log_lik(mu_hat, sigma_hat)\n",
    "    loss.backward() # Gradient computed.\n",
    "    opt.step()      # Update parameter values using gradient descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: mu =  5.195e-02, sigma =  9.651e-02\n"
     ]
    }
   ],
   "source": [
    "print('Parameters: mu = {:10.3e}, sigma = {:10.3e}'.format(\n",
    "    mu_hat.detach().numpy()[0], sigma_hat.detach().numpy()[0])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the asymptotic distribution of the ML estimator is given by\n",
    "\n",
    "$$\n",
    "    \\sqrt{N} \\left(\\widehat\\boldsymbol{\\theta} - \\boldsymbol{\\theta} \\right) \\;\\; \\underset{d}{\\longrightarrow} \\;\\; \\mathcal{N}\\left( \\boldsymbol{0} \\, , \\, \\mathbf{I}(\\boldsymbol{\\theta})^{-1} \\right) \\;\\; \\Longrightarrow \\;\\; \\widehat\\boldsymbol{\\theta} \\; \\underset{d}{\\sim} \\;\\; \\mathcal{N}\\left( \\boldsymbol{\\theta} \\, , \\, \\frac{\\mathbf{I}(\\boldsymbol{\\theta})^{-1}}{N}  \\right)\n",
    "$$\n",
    "\n",
    "Since the Fisher information matrix is the inverse of the Hessian of the log-likelihood, we can use `torch.autograd.functional.hessian` to derive the Hessian matrix, which yields the information matrix and then the standard errors by taking the square root of the diagonal elements."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "theta_hat = (mu_hat, sigma_hat)\n",
    "\n",
    "# Fisher Information matrix.\n",
    "I = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta_hat))\n",
    "\n",
    "# Compute variance matrix.\n",
    "V = torch.inverse(I)/N\n",
    "\n",
    "# Compute standard errors.\n",
    "std_err = np.sqrt(np.diag(V))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Errors: mu =  9.651e-05, sigma =  6.626e-05\n"
     ]
    }
   ],
   "source": [
    "print('Standard Errors: mu = {:10.3e}, sigma = {:10.3e}'.format(std_err[0], std_err[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}