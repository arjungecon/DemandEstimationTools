{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical Optimization using PyTorch\n",
    "\n",
    "In this Jupyter notebook, I perform some basic econometric optimization routines using the PyTorch package.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributions as td\n",
    "import mdmm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Basic Optimization Problem\n",
    "\n",
    "Consider the following problem:\n",
    "\n",
    "$$\n",
    "    \\min_{x} \\quad 2 x^2 - 7 x + 6\n",
    "$$\n",
    "\n",
    "We use the `torch.tensor` method to store parameters, constants and matrices. However, the parameters over which we are optimizing the objective function require an additional option `requires_grad=True` so that PyTorch knows how to perform the backpropagation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical optimization solution: tensor([1.7500], requires_grad=True)\n",
      "Analytic optimization solution: 1.75\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "optimizer = optim.Adam([x], lr=0.05)\n",
    "\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = (2 * x**2 - 7 * x + 6)\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Numerical optimization solution:\", x)\n",
    "print(\"Analytic optimization solution:\", 7/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`optimizer.step()` moves the parameter in the direction that minimizes the objective function using the gradient computed in the `.backward()` operation.\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "## Maximum Likelihood Estimation Part 1\n",
    "\n",
    "In this example, consider some observations $\\{X_i \\mid i = 1, \\ldots, N\\}$ drawn from a normal distribution with mean $\\mu$ and variance $\\sigma^2$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "N = 1000\n",
    "μ, σ = 0.05, 0.1\n",
    "x_data = μ + torch.randn(N) * σ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the ML estimates maximize the log-likelihood function, or correspondingly minimize the negative log-likelihood. We can set the mean and standard deviation as PyTorch `Variable`s and ask the optimizer to minimize the objective accordingly.\n",
    "\n",
    "$$\n",
    "    \\widehat{\\boldsymbol{\\theta}} \\; \\equiv \\; \\begin{pmatrix} \\widehat{\\mu} \\\\ \\widehat{\\sigma} \\end{pmatrix} \\quad \\in \\;\\; \\underset{\\begin{pmatrix} \\mu \\\\ \\sigma \\end{pmatrix}}{\\arg \\min} -\\frac{N}{2} \\log (2 \\pi) - N \\log \\sigma - \\frac{1}{2} \\sum_{i = 1}^N  {\\left( \\frac{X_i - \\mu}{\\sigma} \\right)}^2\n",
    "$$\n",
    "\n",
    "I used `torch.distribution`'s inbuilt log-likelihood method corresponding to a Normal distribution to define the sum of the log-likelihoods over the set of observations.\n",
    "\n",
    "The subpackage `torch.optim` implements various optimization algorithms. To construct an `Optimizer`, we provide an iterable containing the parameters (here, $\\mu$ and $\\sigma$) to optimize. Then, we can specify optimizer-specific options such as the learning rate, weight decay, etc. I chose the Adam algorithm with a small learning rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set parameters for the ML optimization.\n",
    "mu_hat = Variable(torch.zeros(1), requires_grad=True)\n",
    "sigma_hat = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "# Define the objective function.\n",
    "def log_lik(mu, sigma):\n",
    "\n",
    "    return td.Normal(loc=mu, scale=sigma).log_prob(x_data).sum()\n",
    "\n",
    "# Define the adaptive gradient descent optimizer used to find the estimates.\n",
    "opt = optim.Adam([mu_hat, sigma_hat], lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "\n",
    "    opt.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss = - log_lik(mu_hat, sigma_hat)\n",
    "    loss.backward() # Gradient computed.\n",
    "    opt.step()      # Update parameter values using gradient descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: mu =  4.852e-02, sigma =  9.745e-02\n"
     ]
    }
   ],
   "source": [
    "print('Parameters: mu = {:10.3e}, sigma = {:10.3e}'.format(\n",
    "    mu_hat.detach().numpy()[0], sigma_hat.detach().numpy()[0])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the asymptotic distribution of the ML estimator is given by\n",
    "\n",
    "$$\n",
    "    \\sqrt{N} \\left(\\widehat\\boldsymbol{\\theta} - \\boldsymbol{\\theta} \\right) \\;\\; \\underset{d}{\\longrightarrow} \\;\\; \\mathcal{N}\\left( \\boldsymbol{0} \\, , \\, \\mathbf{I}(\\boldsymbol{\\theta})^{-1} \\right) \\;\\; \\Longrightarrow \\;\\; \\widehat\\boldsymbol{\\theta} \\; \\underset{d}{\\sim} \\;\\; \\mathcal{N}\\left( \\boldsymbol{\\theta} \\, , \\, \\frac{\\mathbf{I}(\\boldsymbol{\\theta})^{-1}}{N}  \\right)\n",
    "$$\n",
    "\n",
    "Since the Fisher information matrix is the inverse of the Hessian of the log-likelihood, we can use `torch.autograd.functional.hessian` to derive the Hessian matrix, which yields the information matrix and then the standard errors by taking the square root of the diagonal elements."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "theta_hat = (mu_hat, sigma_hat)\n",
    "\n",
    "# Fisher Information matrix.\n",
    "info_mle = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta_hat))\n",
    "\n",
    "# Compute variance matrix.\n",
    "var_mle = torch.inverse(info_mle)/N\n",
    "\n",
    "# Compute standard errors.\n",
    "std_mle = np.sqrt(np.diag(var_mle))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Errors: mu =  9.745e-05, sigma =  6.955e-05\n"
     ]
    }
   ],
   "source": [
    "print('Standard Errors: mu = {:10.3e}, sigma = {:10.3e}'.format(std_mle[0], std_mle[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------------------------------\n",
    "\n",
    "## Ordinary Least Squares Estimation\n",
    "\n",
    "In this next example, consider some observations $\\{Y_i, \\mathbf{X}_i \\mid i = 1, \\ldots, N\\}$ corresponding to the following DGP:\n",
    "\n",
    "$$\n",
    "    Y_i \\; = \\; \\mathbf{X}_i \\boldsymbol{\\beta} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where we assume that $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "I simulate a dataset with 3 covariates below, and will attempt to recover the coefficients of the linear using the OLS objective function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "## Simulation of data corresponding to the DGP above\n",
    "\n",
    "mean_X = torch.tensor([5.0, -3.0, 2.0])\n",
    "cov_X = torch.tensor(\n",
    "    [[4.0, 1.6, -2.4],\n",
    "     [1.6, 1.0, -0.9],\n",
    "     [-2.4, -0.9, 9.0]]\n",
    ")\n",
    "N = 1000\n",
    "sigma_eps = 0.9\n",
    "\n",
    "X = td.MultivariateNormal(\n",
    "    loc=mean_X, covariance_matrix=cov_X).sample((N,))\n",
    "eps = td.Normal(loc=0, scale=sigma_eps).sample((N,))\n",
    "\n",
    "# Add a column of ones\n",
    "X = torch.column_stack((torch.ones(N,), X))\n",
    "\n",
    "# Choose linear model coefficients\n",
    "beta = torch.tensor([10, -1.0, 0.5, 2.0])\n",
    "\n",
    "y = X @ beta + eps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Parameter and objective function for OLS\n",
    "\n",
    "beta_hat = Variable(torch.zeros(4), requires_grad=True)\n",
    "\n",
    "def ols_loss(b):\n",
    "    return torch.nn.MSELoss()(X @ b, y)\n",
    "\n",
    "opt_ols = optim.Adam([beta_hat], lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Optimizing over the OLS loss function\n",
    "\n",
    "for epoch in range(5000):\n",
    "\n",
    "    opt_ols.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss = ols_loss(beta_hat)\n",
    "    loss.backward() # Gradient computed.\n",
    "    opt_ols.step()      # Update parameter values using gradient descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([10.318044 , -1.0214003,  0.5652391,  1.9927149], dtype=float32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the minimizer values\n",
    "\n",
    "np.array(beta_hat.detach())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a sanity check, we can check the results obtained using the standard regression package `statsmodels`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([10.318071 , -1.021403 ,  0.5652436,  1.9927149], dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(np.array(y), np.array(X)).fit().params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similary, we can compute and compare the standard errors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "ols_r = (X @ beta_hat - y).detach()\n",
    "sig2_hat = (ols_r @ ols_r.T)/(X.shape[0] - X.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.2442146 , 0.02346404, 0.04540195, 0.00982629], dtype=float32)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.diag(sig2_hat * torch.inverse(X.T @ X)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.24421541, 0.02346409, 0.04540214, 0.00982628], dtype=float32)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(np.array(y), np.array(X)).fit().bse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------------------------------\n",
    "\n",
    "## Constrained Optimization\n",
    "\n",
    "$$\n",
    "    \\min_{x, y} \\;\\; 5x - 3 y \\quad \\text{s.t.} \\;\\; x^2 + y^2 = 136\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical optimization solution: x = tensor([-545.6191], requires_grad=True) and y = tensor([560.3257], requires_grad=True)\n",
      "Analytic optimization solution: x = -10 and y = 6\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for the constrained optimization.\n",
    "x = Variable(torch.zeros(1), requires_grad=True)\n",
    "y = Variable(torch.ones(1), requires_grad=True)\n",
    "λ = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "opt_con = optim.Adam([x, y], lr=0.05)\n",
    "\n",
    "for _ in range(10000):\n",
    "    opt_con.zero_grad()\n",
    "    objective = 5*x - 3*y - 0.01 * (x ** 2 + y ** 2 - 136)\n",
    "    objective.backward()\n",
    "    opt_con.step()\n",
    "\n",
    "print(\"Numerical optimization solution: x = {} and y = {}\".format(x, y))\n",
    "print(\"Analytic optimization solution: x = -10 and y = 6\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-499.4106], requires_grad=True)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python396jvsc74a57bd0bf88eea2e90ce10868d34ddfb17a2387af41682c0f3beb37e979d3e0307e8027",
   "language": "python",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}