{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pyblp as blp\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from linearmodels.iv import IV2SLS\n",
    "from HomogenousDemandEstimation import HomDemEst\n",
    "from GaussHermiteQuadrature import GaussHermiteQuadrature\n",
    "\n",
    "blp.options.digits = 2\n",
    "blp.options.verbose = False\n",
    "nax = np.newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `ps1_ex4.csv` contains aggregate data on $T=600$ markets in which $J=6$ products compete between each other together with an outside good $j=0$. The utility of consumer $i$ is given by:\n",
    "\n",
    "$$\n",
    "u_{i j t} \\; = \\; \\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\beta} + \\xi_{j t}+\\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\Gamma} \\boldsymbol{v}_{i}+\\epsilon_{i j t} \\quad j=1, \\ldots, 6 \\\\\n",
    "u_{i 0 t} \\; = \\; \\epsilon_{i 0 t}\n",
    "$$\n",
    "\n",
    "where $x_{j t}$ is a vector of observed product characteristics including the price, $\\xi_{j t}$ is an unobserved product characteristic, $v_{i}$ is a vector of unobserved taste shocks for the product characteristics and $\\epsilon_{i j t}$ is i.i.d T1EV $(0,1)$. Our goal is to to estimate demand parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Gamma})$ using the BLP algorithm. As you can see from the data there are only two characteristics $\\widetilde{\\mathbf{x}}_{j t}=\\begin{pmatrix} p_{j t} & x_{j t} \\end{pmatrix}$, namely prices and an observed measure of product quality. Moreover, there are several valid instruments $\\mathbf{z}_{j t}$ that you will use to construct moments to estimate $(\\boldsymbol{\\beta}, \\boldsymbol{\\Gamma})$. Finally, you can assume that $\\Gamma$ is lower triangular e.g.,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Gamma} \\; = \\; \\begin{pmatrix}\n",
    "\\gamma_{11} & 0 \\\\\n",
    "\\gamma_{21} & \\gamma_{22}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "such that $\\boldsymbol{\\Gamma} \\boldsymbol{\\Gamma}^{\\prime}=\\boldsymbol{\\Omega}$ is a positive definite matrix and that $v_{i}$ is a 2 dimensional vector of i.i.d random taste shocks distributed $\\mathcal{N}\\left(\\mathbf{0}, \\mathbf{I}_2 \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "data_ex4 = pd.read_csv('ps1_ex4.csv')\n",
    "data_ex4['const'] = 1.0     # Add a constant term\n",
    "\n",
    "num_prod = data_ex4.choice.nunique()   # Number of products to choose from.\n",
    "num_T = data_ex4.market.nunique()\n",
    "\n",
    "# Create outside option shares and merge into dataset.\n",
    "share_total = data_ex4.groupby(['market'])['shares'].sum().reset_index()\n",
    "share_total.rename(columns={'shares': 's0'}, inplace=True)\n",
    "share_total['s0'] = 1 - share_total['s0']\n",
    "data_ex4 = pd.merge(data_ex4, share_total, on='market')\n",
    "\n",
    "# Create natural log of share ratios\n",
    "data_ex4['sr'] = np.log(data_ex4['shares']/data_ex4['s0'])\n",
    "\n",
    "# Create constant term\n",
    "data_ex4['const'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The market shares can be expressed as a function of individual characteristics as shown below.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_{j} \\; & \\simeq \\; \\mathbb{E}[\\operatorname{Pr}(i \\text { Chooses } j)] \\\\\n",
    "&= \\; \\int_{\\mathbf{v}_{i}} \\; \\operatorname{Pr}(i \\text { Chooses } j)\n",
    "\\; \\text{d}\\,  F\\left(\\mathbf{v}_{i}\\right) \\\\\n",
    "&= \\; \\int_{\\mathbf{v}_{i}} \\; \\frac{ \\exp \\left(  \\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\beta} + \\xi_{j t}+\\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\Gamma} \\boldsymbol{v}_{i} \\right) }{1 + \\sum_{k \\in \\mathcal{J}_{t}} \\exp \\left(  \\widetilde{\\mathbf{x}}_{k t}^{\\prime} \\boldsymbol{\\beta} + \\xi_{k t}+\\widetilde{\\mathbf{x}}_{k t}^{\\prime} \\boldsymbol{\\Gamma} \\boldsymbol{v}_{i} \\right)} \\text{d}\\,  F\\left(\\mathbf{v}_{i}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, due to the heterogeneity in individual preferences, we do not have a neat solution to back out the preference parameters from using logarithms of share-ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9158,  0.7115, -0.3054], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain initial guess for Î² using the homogenous model.\n",
    "\n",
    "est = HomDemEst(data_dict={\n",
    "    'Data': data_ex4,\n",
    "    'Choice Column': 'choice',\n",
    "    'Market Column': 'market',\n",
    "    'Log Share Ratio Column': 'sr',\n",
    "    'Endogenous Columns': ['p'],\n",
    "    'Exogenous Columns': ['x'],\n",
    "    'Instrument Columns': ['z1', 'z2', 'z3', 'z4', 'z5', 'z6'],\n",
    "    'Add Constant': True\n",
    "})\n",
    "\n",
    "beta_guess = torch.tensor(np.array(est.one_step_gmm().detach()), dtype=torch.double)\n",
    "beta_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4525, 2.1547],\n",
      "        [0.2082, 0.1282]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for the optimization procedure.\n",
    "gamma = Variable(3 * torch.rand((2,2), dtype=torch.double), requires_grad=True)\n",
    "beta = Variable(beta_guess, requires_grad=False)\n",
    "xi = Variable()\n",
    "\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ghq = GaussHermiteQuadrature(2, 9)\n",
    "ghq_node_mat = ghq.X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save data as Pytorch tensors.\n",
    "shares = torch.tensor(np.array(data_ex4['shares']),\n",
    "                      dtype=torch.double)\n",
    "covars = torch.tensor(np.array(data_ex4[['const', 'x', 'p']]),\n",
    "                      dtype=torch.double)\n",
    "num_covar = covars.size()[1]\n",
    "instruments = torch.tensor(np.array(data_ex4[['const', 'x', 'z1', 'z2',\n",
    "                                              'z3', 'z4', 'z5', 'z6']]),\n",
    "                                      dtype=torch.double)\n",
    "x_mat = covars.reshape((num_T, num_prod, num_covar))\n",
    "s_mat = shares.reshape((num_T, num_prod))\n",
    "\n",
    "x_random_mat = x_mat[:, :, 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Nested Fixed Point Approach\n",
    "\n",
    "We solve for the model parameters $\\boldsymbol{\\beta}$ and $\\Gamm$ using the NFXP algorithm outlined in BLP (1995) and Nevo (2001).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_utility(b, xi):\n",
    "\n",
    "    return covars @ b[:, None] + xi\n",
    "\n",
    "def market_share_val(delta, g):\n",
    "\n",
    "    # Evaluate the expression for every market, product and\n",
    "    # Gauss-Hermite node.\n",
    "    # Returns a matrix of size (num_T, num_prod, GHQ_size).\n",
    "\n",
    "    numer = torch.exp(delta[:, :, None] + torch.einsum('tjk,kl,lm -> tjm', x_random_mat, g, ghq_node_mat))\n",
    "    denom = 1 + numer.sum(axis=1)\n",
    "\n",
    "    # Compute the share matrix for every value of unobserved individual characteristics.\n",
    "    share_mat = numer.div(denom[:, None])\n",
    "\n",
    "    # Take the expected value of the above matrix using a GH integral\n",
    "    # approximation.\n",
    "    exp_share = torch.einsum('m, tjm -> tj', ghq.W, share_mat)\n",
    "\n",
    "    return exp_share\n",
    "\n",
    "def blp_contraction(b, g, res):\n",
    "\n",
    "    # Initial guess for mean utility\n",
    "    delta = mean_utility(b, res).reshape((num_T, num_prod))\n",
    "\n",
    "    error, tol = 1, 1e-12\n",
    "\n",
    "    while error > tol:\n",
    "\n",
    "        exp_delta_new = torch.exp(delta) * s_mat.div(market_share_val(delta, g))\n",
    "\n",
    "        delta_new = torch.log(exp_delta_new)\n",
    "\n",
    "        error = torch.linalg.norm(delta_new - delta)\n",
    "        delta = delta_new\n",
    "\n",
    "        if error % 20 == 0:\n",
    "            print('Inner Loop Error = {}'.format(error))\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def blp_gmm_loss(b, g):\n",
    "\n",
    "    xi = torch.zeros((num_prod * num_T, 1), dtype=torch.double, requires_grad=False)\n",
    "\n",
    "    # Obtaining the BLP contraction solution for the mean utilities.\n",
    "    delta = blp_contraction(b, g, xi).reshape((num_T * num_prod, 1))\n",
    "\n",
    "    # Run 2SLS of mean utilities on covariates (including prices).\n",
    "    blp_2sls = IV2SLS(dependent=np.array(delta.detach()),\n",
    "                      exog=data_ex4[['const', 'x']],\n",
    "                      endog=data_ex4['p'],\n",
    "                      instruments=data_ex4[['z1', 'z2', 'z3', 'z4', 'z5', 'z6']]).fit()\n",
    "\n",
    "    # Use 2SLS coefficients.\n",
    "    b_2sls = torch.tensor(np.array(blp_2sls.params))\n",
    "\n",
    "    # Derive residuals using 2SLS coefficients.\n",
    "    res = delta - covars @ b_2sls[:, None]\n",
    "\n",
    "    # Derive moment conditions required for BLP.\n",
    "    moment_eqns = res * instruments\n",
    "    moments = moment_eqns.mean(axis=0)\n",
    "\n",
    "    loss_gmm = moments[None, :] @ weight_matrix @ moments[:, None]\n",
    "\n",
    "    print('beta = {}, gamma = {}, loss = {}'.format(np.array(b_2sls.clone().detach()),\n",
    "                                                    np.array(g.clone().detach()),\n",
    "                                                    loss_gmm.clone().detach())\n",
    "          )\n",
    "\n",
    "    return loss_gmm, moment_eqns, b_2sls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "opt_gmm = optim.Adam([gamma], lr=0.01)\n",
    "weight_matrix = Variable(torch.eye(instruments.shape[1], dtype=torch.double), requires_grad=False)\n",
    "\n",
    "# Optimizing over the GMM loss function\n",
    "for epoch in range(500):\n",
    "\n",
    "    opt_gmm.zero_grad()   # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss, moment_x, new_beta = blp_gmm_loss(beta, gamma)\n",
    "    loss.backward()    # Gradient computed.\n",
    "\n",
    "    opt_gmm.step()     # Update parameter values using gradient descent.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gamma[0,1] = gamma[0,1].clamp(0.00, 0.00)\n",
    "        beta[1] = beta[1].clamp(0.00, np.inf)\n",
    "        # gamma[0,0] = gamma[0,0].clamp(0.00, np.inf)\n",
    "        # gamma[1,1] = gamma[1,1].clamp(0.00, np.inf)\n",
    "\n",
    "    weight_matrix = torch.inverse(1/(num_T * num_prod) * (moment_x.T @ moment_x).detach())\n",
    "    beta = new_beta.detach()\n",
    "    # beta = beta2.detach().clone()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    #\n",
    "    #     loss_val = np.squeeze(loss.detach())\n",
    "    #     print('Iteration [{}]: Loss = {:2.4e}'.format(epoch, loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0276,  0.0000],\n",
       "        [-0.2168,  0.1702]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.5439,  0.5183, -0.3859], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.1112315 , -0.43952497],\n",
       "       [-0.43952497,  0.07597359]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ï = np.array((gamma @ gamma.T).detach())\n",
    "Ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7864411888049343"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ï[1,0] / np.sqrt(Ï[0,0] * Ï[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0276172 , 0.27563308])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt([Ï[0,0] , Ï[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that\n",
    "\n",
    "$$\n",
    "    \\widehat{\\boldsymbol{\\beta}} \\; = \\; \\begin{pmatrix} -4.5439 \\\\  0.5183 \\\\ -0.3859 \\end{pmatrix}, \\qquad \\widehat{\\boldsymbol{\\Gamma}} \\; = \\; \\begin{pmatrix} 2.0276 & 0.0000 \\\\ -0.2168 & 0.1702\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "beta, gamma = beta.detach(), gamma.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "To compute market-specific elasticities, we need to first predict individual level market shares for various realizations of $\\mathbf{v}_i$ and then average these across all individuals. For each realization of $\\mathbf{v}_i$, the predicted market share for product $j$ is given by\n",
    "\n",
    "$$\n",
    "s_{ijt} \\; = \\; \\frac{ \\exp \\left(  \\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\beta} + \\xi_{j t}+\\widetilde{\\mathbf{x}}_{j t}^{\\prime} \\boldsymbol{\\Gamma} \\boldsymbol{v}_{i} \\right) }{1 + \\sum_{k \\in \\mathcal{J}_{t}} \\exp \\left(  \\widetilde{\\mathbf{x}}_{k t}^{\\prime} \\boldsymbol{\\beta} + \\xi_{k t}+\\widetilde{\\mathbf{x}}_{k t}^{\\prime} \\boldsymbol{\\Gamma} \\boldsymbol{v}_{i} \\right)}\n",
    "$$\n",
    "\n",
    "The individual coefficients are given by\n",
    "\n",
    "$$\n",
    "    \\widehat{\\boldsymbol{\\beta}}_i \\; = \\; \\widehat{\\boldsymbol{\\beta}} + \\boldsymbol{\\Gamma} \\boldsymbol{v_i}\n",
    "$$\n",
    "\n",
    "We can put these together to compute the own-price and cross-price elasticities for each market using the following equations:\n",
    "\n",
    "$$\n",
    "\\varepsilon_{j k, t} \\; = \\; \\frac{\\partial \\pi_{j, t}}{\\partial p_{k, t}} \\frac{p_{k, t}}{\\pi_{j, t}} \\; = \\; \\begin{cases}-\\frac{p_{j, t}}{\\pi_{j, t}} \\int_{\\mathbf{v}_{i}}  \\alpha_{i} \\pi_{i, j, t}\\left(1-\\pi_{i, j, t}\\right) \\text{d} F\\left(\\mathbf{v}_{i}\\right) \\quad & \\text { if } j=k \\\\\n",
    "\\frac{p_{k, t}}{\\pi_{j, t}} \\int_{\\mathbf{v}_{i}}   \\alpha_{i} \\pi_{i, j, t} \\pi_{i, k, t} \\; \\text{d} F\\left(\\mathbf{v}_{i}\\right) \\quad & \\text { otherwise. }\\end{cases}\n",
    "$$\n",
    "\n",
    "We again rely on Gauss-Hermite quadratures to evaluate the integrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_ind_params(b, g):\n",
    "\n",
    "    # Predicts individual market shares and individual price coefficients for each product at every GH node.\n",
    "    # Returns a matrix of size (num_T, num_prod, GHQ_size) and (2, GHQ_size)\n",
    "\n",
    "    numer = torch.exp(torch.einsum('tjk,kl->tjl', x_mat, b[:, None]) + torch.einsum('tjk,kl,lm -> tjm', x_random_mat, g, ghq_node_mat))\n",
    "    denom = 1 + numer.sum(axis=1)\n",
    "\n",
    "    # Compute the share matrix for every value of unobserved individual characteristics.\n",
    "    share_mat = numer.div(denom[:, None])\n",
    "\n",
    "    beta_mat = b[1:-1][:, None] + torch.einsum('kl,lm -> km', g, ghq_node_mat)\n",
    "\n",
    "    return share_mat, beta_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_wide = pd.pivot(data_ex4, values='p', index='market', columns='choice')\n",
    "\n",
    "data_wide.rename(columns={1: \"price_1\", 2: \"price_2\",\n",
    "                                      3: \"price_3\", 4: \"price_4\",\n",
    "                                      5: \"price_5\", 6: \"price_6\"}, inplace=True)\n",
    "\n",
    "data_ex4 = pd.merge(data_ex4, data_wide, on='market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "beta[-1] = -beta[-1]\n",
    "\n",
    "share_mat, beta_mat = generate_ind_params(beta, gamma)\n",
    "\n",
    "own_price_integral = torch.einsum('tjm, tjm, m, m -> tj', share_mat, 1 - share_mat, beta_mat[1,:], ghq.W)\n",
    "\n",
    "cross_price_integral = torch.einsum('tjm, tkm, m, m -> tjk', share_mat, share_mat, beta_mat[1,:], ghq.W)\n",
    "\n",
    "data_ex4['own'] = - own_price_integral.reshape((num_T * num_prod)) * data_ex4['p'] / data_ex4['shares']\n",
    "\n",
    "data_ex4['cross_1'] = cross_price_integral[:, :, 0].reshape((num_T * num_prod)) * data_ex4['price_1'] / data_ex4['shares']\n",
    "data_ex4['cross_2'] = cross_price_integral[:, :, 1].reshape((num_T * num_prod)) * data_ex4['price_2'] / data_ex4['shares']\n",
    "data_ex4['cross_3'] = cross_price_integral[:, :, 2].reshape((num_T * num_prod)) * data_ex4['price_3'] / data_ex4['shares']\n",
    "data_ex4['cross_4'] = cross_price_integral[:, :, 3].reshape((num_T * num_prod)) * data_ex4['price_4'] / data_ex4['shares']\n",
    "data_ex4['cross_5'] = cross_price_integral[:, :, 4].reshape((num_T * num_prod)) * data_ex4['price_5'] / data_ex4['shares']\n",
    "data_ex4['cross_6'] = cross_price_integral[:, :, 5].reshape((num_T * num_prod)) * data_ex4['price_6'] / data_ex4['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "average_elasticity = data_ex4.groupby('choice')[['own', 'cross_1', 'cross_2', 'cross_3', 'cross_4', 'cross_5', 'cross_6']].mean()\n",
    "e_mat = np.array(average_elasticity[['cross_1', 'cross_2', 'cross_3', 'cross_4', 'cross_5', 'cross_6']])\n",
    "np.fill_diagonal(e_mat, np.array(average_elasticity['own']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "| j/k       |   Product 1 |    Product 2 | Product 3 | Product 4 | Product 5 | Product 6 |\n",
    "|:----------|------------:|-------------:|----------:|----------:|----------:|----------:|\n",
    "| Product 1 | -0.00058851 |  1.01873e-05 | 0.0383455 |  0.023619 |  0.111892 |  0.103909 |\n",
    "| Product 2 |  2.5978e-05 | -0.000799222 | 0.0416366 | 0.0259857 |  0.157375 |  0.210676 |\n",
    "| Product 3 | 0.000215098 |  0.000272143 |  -21.8507 |   4.99784 |   4.66744 |   4.99098 |\n",
    "| Product 4 | 0.000215119 |  0.000143814 |  0.609505 |    -18.32 |   4.39132 |   4.94391 |\n",
    "| Product 5 | 6.38342e-05 |  6.19015e-05 |  0.273356 |  0.288736 |  -5.41066 |    2.6977 |\n",
    "| Product 6 | 4.42949e-05 |  7.32514e-05 |  0.215868 |  0.197864 |   2.42652 |  -5.01205 |\n",
    "\n",
    "$$\\quad\n",
    "$$\n",
    "\n",
    "We see that own price and cross price elasticities are not driven solely by functional form, but by the heterogeneity in the price\n",
    "sensitivity across consumers who purchase the various products. This creates the difference between the results here and in Exercise 3. The absurdly low elasticities associated with products 1 and 2 could be driven by the extremely low prices for these products across all markets as seen in the table below for Part 3.\n",
    "\n",
    "$$\n",
    "    \\quad\n",
    "$$\n",
    "\n",
    "### Part 3\n",
    "\n",
    "The difference in prices and market shares could be attributed to certain products having much lower quality on average (especially products 3 and 4) compared to products 5 and 6. The impact of quality on customer preferences might be heterogenous, but the coefficient related to quality is strictly positive with low variance, which implies that customers will tend to shift away from these products in unison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>x</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002439</td>\n",
       "      <td>-0.019330</td>\n",
       "      <td>0.098810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002286</td>\n",
       "      <td>-0.026036</td>\n",
       "      <td>0.089131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.019113</td>\n",
       "      <td>-0.081252</td>\n",
       "      <td>0.043009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.751616</td>\n",
       "      <td>-0.180135</td>\n",
       "      <td>0.039323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.576978</td>\n",
       "      <td>1.692610</td>\n",
       "      <td>0.151714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.442894</td>\n",
       "      <td>2.002366</td>\n",
       "      <td>0.193238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               p         x    shares\n",
       "choice                              \n",
       "1       0.002439 -0.019330  0.098810\n",
       "2       0.002286 -0.026036  0.089131\n",
       "3       2.019113 -0.081252  0.043009\n",
       "4       1.751616 -0.180135  0.039323\n",
       "5       3.576978  1.692610  0.151714\n",
       "6       4.442894  2.002366  0.193238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ex4.groupby('choice')[['p', 'x', 'shares']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}