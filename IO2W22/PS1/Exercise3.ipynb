{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbbfb4f-74f6-4253-bb83-dbe6f7ae6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pyblp as blp\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "blp.options.digits = 2\n",
    "blp.options.verbose = False\n",
    "nax = np.newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f7343-a0de-4919-b26d-2a3b23c49f52",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The file `ps1_ex3.csv` contains aggregate data on a large number $T=1000$ of markets in which $J=6$ products compete between each other together with an outside good $j=0$. The utility of consumer $i$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&u_{i j t}=-\\alpha p_{j t}+\\beta x_{j t}+\\xi_{j t}+\\epsilon_{i j t} \\quad j=1, \\ldots, 6 \\\\\n",
    "&u_{i 0 t}=\\epsilon_{i 0 t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_{j t}$ is the price of product $j$ in market $t, x_{j t}$ is an observed product characteristic, $\\xi_{j t}$ is an unobserved product characteristic and $\\epsilon_{i j t}$ is i.i.d T1EV $(0,1)$. Our goal is to to estimate demand parameters $(\\alpha, \\beta)$ and perform some counterfactual exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed81671-ea13-428b-99bd-5741b2eed279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "data_ex3 = pd.read_csv('ps1_ex3.csv')\n",
    "num_prod = data_ex3.Product.max()\n",
    "num_T = data_ex3.market.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d33cb-f2de-490e-89b3-234273bde825",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Assuming that the variable $z$ in the dataset is a valid instrument for prices, write down the moment condition that allows you to consistently estimate $(\\alpha, \\beta)$ and obtain an estimate for both parameters.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Under the T1EV assumption, we can derive the the con which corresponds to the predicted market share for product $j$ at time $t$. This can be approximated from the data using the observed market share $s_{jt}$.\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}(i \\text{ chooses }j \\text{ at time } t) \\; = \\; \\frac{\\exp \\left(-\\alpha p_{jt}+{x}_{jt} \\beta + \\xi_{jt}\\right)}{\\sum_{k \\in \\mathcal{J}_{t}} \\exp \\left(-\\alpha p_{kt}+{x}_{kt} \\beta+\\xi_{kt}\\right)} \\; \\approx \\; s_{jt}\n",
    "$$\n",
    "\n",
    "We can invoke the normalization assumption on $u_{i0}$ and take the logarithm of the share ratio $s_{jt}/s_{0t}$ to obtain\n",
    "\n",
    "$$\n",
    "    \\ln \\left({\\frac{s_{jt}}{s_{0t}}\\right) \\; = \\; -\\alpha p_{jt}+ {x}_{jt} \\beta+\\xi_{jt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Create outside option shares and merge into dataset.\n",
    "\n",
    "share_total = data_ex3.groupby(['market'])['Shares'].sum().reset_index()\n",
    "share_total.rename(columns={'Shares': 's0'}, inplace=True)\n",
    "share_total['s0'] = 1 - share_total['s0']\n",
    "data_ex3 = pd.merge(data_ex3, share_total, on='market')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbc1e28-fa53-450d-b237-ecf2883a41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create natural log of share ratios\n",
    "\n",
    "data_ex3['s_ratio'] = np.log(data_ex3['Shares']/data_ex3['s0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that $z_{jt}$ is a relevant instrument for $p_{jt}$ and that $x_{jt}$ is exogenous, we can impose the conditional exogeneity restriction\n",
    " $$\n",
    "    \\mathbb{E}\\left[\\xi_{jt} \\mid x_{jt}, z_{jt} \\right] = 0\n",
    " $$\n",
    "  in order to estimate $\\alpha$ and $\\beta$. Using the Law of Iterated Expectations, we can conclude that\n",
    "\n",
    "$$\n",
    "    \\mathbb{E} \\left[ \\begin{pmatrix} x_{jt} \\\\ z_{jt} \\end{pmatrix} \\xi_{jt} \\right] \\; = \\;  \\mathbb{E} \\left[ \\begin{pmatrix} x_{jt} \\\\ z_{jt} \\end{pmatrix} \\left\\{\\ln \\left({\\frac{s_{jt}}{s_{0t}}\\right) + \\alpha p_{jt} - {x}_{jt} \\beta \\right\\}\\right] \\; = \\; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Given that we have $6$ products and $2$ moment conditions for each product, we are over-identifying $\\alpha$ and $\\beta$.\n",
    "\n",
    "GMM provides the minimizer corresponding to a quadratic loss function with 12 moments.\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix} \\widehat{\\alpha} \\\\ \\widehat{\\beta} \\end{pmatrix} \\quad \\in \\;\\; \\underset{\\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}}{\\arg \\min}  \\left[ \\frac{1}{T} \\sum_{t} x_{jt} \\left\\{\\ln \\left({\\frac{s_{jt}}{s_{0t}}\\right) + \\alpha p_{jt} - {x}_{jt} \\beta \\right\\} \\right]\n",
    "$$\n",
    "\n",
    "I perform the two-step procedure to obtain the efficient GMM estimator of the model parameters.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Set parameters for the GMM optimization.\n",
    "α = Variable(torch.ones(1), requires_grad=True)\n",
    "β = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "lns = torch.tensor(data_ex3['s_ratio'])\n",
    "p = torch.tensor(data_ex3['Prices'])\n",
    "x = torch.tensor(data_ex3['x'])\n",
    "z = torch.tensor(data_ex3['z'])\n",
    "\n",
    "weight_matrix = torch.eye(num_prod * 2, dtype=torch.double)\n",
    "\n",
    "def ex3_gmm_eqns(a, b):\n",
    "\n",
    "    cond1 = (lns + a * p - b * x) * x\n",
    "    cond2 = (lns + a * p - b * x) * z\n",
    "\n",
    "    return [cond1.reshape((num_T, num_prod)),\n",
    "            cond2.reshape((num_T, num_prod))]\n",
    "\n",
    "# Compile the moments required for GMM.\n",
    "def gmm_moments(a, b):\n",
    "\n",
    "    moments = [eq.mean(axis=0) for eq in ex3_gmm_eqns(a, b)]\n",
    "    return torch.cat(tuple(moments), 0)\n",
    "\n",
    "# Define the objective function for GMM.\n",
    "def gmm_loss(a, b):\n",
    "\n",
    "    moments = gmm_moments(a, b)\n",
    "    return moments[None, :] @ weight_matrix @ moments[:, None]\n",
    "\n",
    "# Define the Jacobian of moments vector G.\n",
    "def return_g(a, b):\n",
    "\n",
    "    g = torch.autograd.functional.jacobian(gmm_moments, (a, b))\n",
    "    return torch.cat(g, 1)\n",
    "\n",
    "# Define the variance of the moments.\n",
    "def return_var_g(a, b):\n",
    "\n",
    "    g = torch.cat(tuple(ex3_gmm_eqns(a, b)), 1)\n",
    "    return 1/num_T * (g.T @ g)\n",
    "\n",
    "# Define the adaptive gradient descent optimizer used to find the estimates.\n",
    "opt_gmm = optim.Adam([α, β], lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Optimizing over the GMM loss function\n",
    "for epoch in range(10000):\n",
    "\n",
    "    opt_gmm.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss_round1 = gmm_loss(a=α, b=β)\n",
    "    loss_round1.backward() # Gradient computed.\n",
    "    opt_gmm.step()     # Update parameter values using gradient descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0.2328], requires_grad=True), tensor([0.2889], requires_grad=True))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α, β"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Set parameters for second round of the GMM optimization.\n",
    "α2 = Variable(torch.ones(1), requires_grad=True)\n",
    "β2 = Variable(torch.ones(1), requires_grad=True)\n",
    "\n",
    "# Construct optimal weight matrix from Step 1.\n",
    "weight_matrix = torch.inverse(return_var_g(α, β)).detach()\n",
    "\n",
    "# Create new optimization object for Step 2 of GMM.\n",
    "opt_gmm2 = optim.Adam([α2, β2], lr=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Optimizing over the GMM loss function\n",
    "for epoch in range(10000):\n",
    "\n",
    "    opt_gmm2.zero_grad() # Reset gradient inside the optimizer\n",
    "\n",
    "    # Compute the objective at the current parameter values.\n",
    "    loss_round2 = gmm_loss(a=α2, b=β2)\n",
    "    loss_round2.backward() # Gradient computed.\n",
    "    opt_gmm2.step()      # Update parameter values using gradient descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0014, 0.0059], dtype=torch.float64)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = return_g(α2, β2).double()\n",
    "V = return_var_g(α2, β2).double()\n",
    "\n",
    "V2 = torch.inverse(G.T @ torch.inverse(V) @ G)\n",
    "V2 = V2.detach()/num_T\n",
    "torch.sqrt(torch.diag(V2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We find the following estimates for $\\alpha$ and $\\beta$.\n",
    "\n",
    "|          | Estimate | Std. Error |\n",
    "|:--------:|:--------:|:----------:|\n",
    "| $\\alpha$ |  0.2342  |   0.0014   |\n",
    "| $\\beta$  |  0.2935  |   0.0059   |\n",
    "\n",
    "We can verify the procedure by comparing the results from running 2SLS on the same sample.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:                s_ratio   R-squared:                      0.7066\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.7065\n",
      "No. Observations:                6000   F-statistic:                 1.338e+04\n",
      "Date:                Wed, Jan 26 2022   P-value (F-stat)                0.0000\n",
      "Time:                        13:30:11   Distribution:                  chi2(2)\n",
      "Cov. Estimator:            unadjusted                                         \n",
      "                                                                              \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "x              0.2876     0.0065     44.586     0.0000      0.2749      0.3002\n",
      "Prices        -0.2396     0.0022    -108.04     0.0000     -0.2440     -0.2353\n",
      "==============================================================================\n",
      "\n",
      "Endogenous: Prices\n",
      "Instruments: z\n",
      "Unadjusted Covariance (Homoskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "iv = IV2SLS(dependent=data_ex3['s_ratio'],\n",
    "       exog=data_ex3['x'],\n",
    "       endog=data_ex3['Prices'],\n",
    "       instruments=data_ex3['z']).fit(cov_type='unadjusted')\n",
    "\n",
    "print(iv.summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part B\n",
    "\n",
    "We know that the elasticities for homogenous demand are given by\n",
    "\n",
    "$$\n",
    "    \\varepsilon_{j k, t} \\; = \\;\n",
    "    \\begin{cases}-\\alpha p_{j, t}\\left(1-\\pi_{j, t}\\right) & \\text { if } j=k \\\\ \\alpha p_{k, t} \\pi_{k, t} & \\text { otherwise }\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "α, β = α2.item(), β2.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Compute the own and cross-price elasticity for each market and pair of products.\n",
    "data_ex3['own'] = -α * data_ex3['Prices'] * (1 - data_ex3['Shares'])\n",
    "data_ex3['cross'] = α * data_ex3['Prices'] * data_ex3['Shares']\n",
    "e_mean = data_ex3.groupby(['Product'])[['own', 'cross']].mean()\n",
    "\n",
    "# Generate matrix of average elasticities.\n",
    "e_mat = np.tile(e_mean['cross'], (num_prod, 1))\n",
    "np.fill_diagonal(e_mat, e_mean['own'])\n",
    "\n",
    "# Convert it to a dataframe.\n",
    "prod_list = list(map(str, range(1, num_prod+ 1)))\n",
    "e_mat = pd.DataFrame(e_mat, index=prod_list, columns=prod_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "          1         2         3         4         5         6\n1 -0.626071  0.161827  0.064499  0.063705  0.062882  0.064863\n2  0.160862 -0.626882  0.064499  0.063705  0.062882  0.064863\n3  0.160862  0.161827 -0.645872  0.063705  0.062882  0.064863\n4  0.160862  0.161827  0.064499 -0.648239  0.062882  0.064863\n5  0.160862  0.161827  0.064499  0.063705 -0.647014  0.064863\n6  0.160862  0.161827  0.064499  0.063705  0.062882 -0.646700",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-0.626071</td>\n      <td>0.161827</td>\n      <td>0.064499</td>\n      <td>0.063705</td>\n      <td>0.062882</td>\n      <td>0.064863</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.160862</td>\n      <td>-0.626882</td>\n      <td>0.064499</td>\n      <td>0.063705</td>\n      <td>0.062882</td>\n      <td>0.064863</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.160862</td>\n      <td>0.161827</td>\n      <td>-0.645872</td>\n      <td>0.063705</td>\n      <td>0.062882</td>\n      <td>0.064863</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.160862</td>\n      <td>0.161827</td>\n      <td>0.064499</td>\n      <td>-0.648239</td>\n      <td>0.062882</td>\n      <td>0.064863</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.160862</td>\n      <td>0.161827</td>\n      <td>0.064499</td>\n      <td>0.063705</td>\n      <td>-0.647014</td>\n      <td>0.064863</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.160862</td>\n      <td>0.161827</td>\n      <td>0.064499</td>\n      <td>0.063705</td>\n      <td>0.062882</td>\n      <td>-0.646700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_mat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results above show that the elasticity "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python396jvsc74a57bd0bf88eea2e90ce10868d34ddfb17a2387af41682c0f3beb37e979d3e0307e8027"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}